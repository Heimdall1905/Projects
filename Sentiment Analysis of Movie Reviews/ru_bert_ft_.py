# -*- coding: utf-8 -*-
"""Ru_BERT_FT_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lJBxOMPc5QzvlL-Gb4nUhO16WeESHFd1
"""

!pip install datasets transformers -q

import pandas as pd
from tqdm.auto import tqdm

from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from datasets import load_dataset, DatasetDict
import transformers

from sklearn.metrics import f1_score, accuracy_score

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

"""**<h2>Классификация тональности отзыва на фильм</h2>**
<h4>Решим задачу определения тональности отзыва, датасет возьмём из базы данных онлайн платформы для просмотра Кинопоиск</h4>

**<h2>Подготовка данных</h2>**
"""

data = load_dataset("blinoff/kinopoisk")
data

print(data['train']['content'][100][:100])
print(data['train']['grade3'][100])

"""<h4>Разделим исходную выборку</h4>"""

dataset = data['train'].train_test_split(test_size=0.2)
test = dataset['test']
train_val = dataset['train'].train_test_split(test_size=0.1)

dataset = DatasetDict({
    'train':train_val['train'],
    'val': train_val['test'],
    'test': test
})

dataset

"""<h4>В качестве модели для дообучения будет взята rubert-tiny2, загрузим её родной токенизатор</h4>"""

model_name = 'cointegrated/rubert-tiny2'

tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)

def preprocess_function(examples):
    model_inputs = tokenizer(examples['content'], max_length=2048, truncation=True)

    labels = {'Good': 2, 'Neutral': 1, 'Bad': 0}
    model_inputs['labels'] = [labels[grade] for grade in examples['grade3']]

    return model_inputs

tokenized_data = dataset.map(preprocess_function, batched=True)

tokenized_data

model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

model

data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)

"""<h4>В течении обучения будем вычислять дополнительные метрики для отчётности после каждой эпохи</h4>"""

def compute_metrics(eval_pred):
  predict, labels = eval_pred
  predict = np.argmax(predict, axis=1)

  accuracy = accuracy_score(labels, predict)
  f1_weighted = f1_score(labels, predict, average='weighted')
  f1_macro = f1_score(labels, predict, average='macro')

  return {'accuracy': accuracy, 'f1_weighted': f1_weighted, 'f1_macro': f1_macro}

training_args = transformers.TrainingArguments(
        output_dir="./results_version_0.1",
        eval_strategy='epoch',
        save_strategy='epoch',
        learning_rate=2e-5,
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        weight_decay=0.1,
        save_total_limit=5,
        num_train_epochs=5,
        report_to = 'none',
        logging_strategy='epoch',
        load_best_model_at_end=True,
        metric_for_best_model='f1_weighted',
        greater_is_better=True,
    )

trainer = transformers.Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_data["train"],
        eval_dataset=tokenized_data["test"],
        processing_class=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics
    )

trainer.train()

model_path = "/content/drive/MyDrive/rubert"

model = transformers.AutoModelForSequenceClassification.from_pretrained(model_path)
tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)

y_pred = trainer.predict(tokenized_data['test'])

y_true = y_pred.label_ids
y_pred_ = np.argmax(y_pred.predictions, axis=1)

print(accuracy_score(y_true, y_pred_))
f1_score(y_true, y_pred_, average='weighted')

"""<h4>Сохраним модель для последующих эксперемнтов</h4>"""

from google.colab import drive
drive.mount('/content/drive')

trainer.save_model("/content/drive/MyDrive/rubert")

tokenizer.save_pretrained("/content/drive/MyDrive/rubert")

